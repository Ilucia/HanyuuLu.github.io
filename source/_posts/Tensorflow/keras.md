---
title: Kerasç®€å•æ ·ä¾‹ä»£ç 
date: 2019-02-26 20:30:33
tags:
---
> ğŸš§æ­£åœ¨æ–½å·¥ä¸­â€¦â€¦ğŸš§
``` py
# https://tensorflow.google.cn/guide/keras
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
print('[tensorflow.version]:', tf.VERSION)
print('[tensorflow.keras.version]:', tf.keras.__version__)

# åºåˆ—æ¨¡å‹
# åœ¨ Keras ä¸­ï¼Œæ‚¨å¯ä»¥é€šè¿‡ç»„åˆå±‚æ¥æ„å»ºæ¨¡å‹ã€‚æ¨¡å‹ï¼ˆé€šå¸¸ï¼‰æ˜¯ç”±å±‚æ„æˆçš„å›¾ã€‚æœ€å¸¸è§çš„æ¨¡å‹ç±»å‹æ˜¯å±‚çš„å †å ï¼štf.keras.Sequential æ¨¡å‹ã€‚
model = tf.keras.Sequential()
# adds a densely-connected layer with 64 units to the model
model.add(layers.Dense(64, activation='relu'))
# add another
model.add(layers.Dense(64, activation='relu'))
# add a softmax layer with 10 output units
model.add(layers.Dense(10, activation='softmax'))

# é…ç½®å±‚
# æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¾ˆå¤š tf.keras.layersï¼Œå®ƒä»¬å…·æœ‰ä¸€äº›ç›¸åŒçš„æ„é€ å‡½æ•°å‚æ•°ï¼š
# activationï¼šè®¾ç½®å±‚çš„æ¿€æ´»å‡½æ•°ã€‚æ­¤å‚æ•°ç”±å†…ç½®å‡½æ•°çš„åç§°æŒ‡å®šï¼Œæˆ–æŒ‡å®šä¸ºå¯è°ƒç”¨å¯¹è±¡ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œç³»ç»Ÿä¸ä¼šåº”ç”¨ä»»ä½•æ¿€æ´»å‡½æ•°ã€‚
# kernel_initializer å’Œ bias_initializerï¼šåˆ›å»ºå±‚æƒé‡ï¼ˆæ ¸å’Œåå·®ï¼‰çš„åˆå§‹åŒ–æ–¹æ¡ˆã€‚æ­¤å‚æ•°æ˜¯ä¸€ä¸ªåç§°æˆ–å¯è°ƒç”¨å¯¹è±¡ï¼Œé»˜è®¤ä¸º "Glorot uniform" åˆå§‹åŒ–å™¨ã€‚
# kernel_regularizer å’Œ bias_regularizerï¼šåº”ç”¨å±‚æƒé‡ï¼ˆæ ¸å’Œåå·®ï¼‰çš„æ­£åˆ™åŒ–æ–¹æ¡ˆï¼Œä¾‹å¦‚ L1 æˆ– L2 æ­£åˆ™åŒ–ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œç³»ç»Ÿä¸ä¼šåº”ç”¨æ­£åˆ™åŒ–å‡½æ•°ã€‚
# Create a sigmoid layer
layers.Dense(64, activation='sigmoid')
# Or
layers.Dense(64, activation=tf.sigmoid)

# a liner layer with L1 regularization of factor 0.01 applied to the kernal matrix
layers.Dense(64, kernel_regularizer=tf.keras.regularizers.l1(0.01))

# A linear layer with L2 regularization of factor 0.01 applied to the bias vector
layers.Dense(64, bias_regularizer=tf.keras.regularizers.l2(0.01))

# A linear layer with a kernel initialized to a random orthogonal matrix
layers.Dense(64, kernel_initializer='orthogonal')

# A linear layer with a bias vector initialized to 2.0s:
layers.Dense(64, bias_initializer=tf.keras.initializers.constant(2.0))

# è®­ç»ƒå’Œè¯„ä¼°
# è®¾ç½®è®­ç»ƒæµç¨‹

# æ„å»ºå¥½æ¨¡å‹åï¼Œé€šè¿‡è°ƒç”¨ compile æ–¹æ³•é…ç½®è¯¥æ¨¡å‹çš„å­¦ä¹ æµç¨‹ï¼š

model = tf.keras.Sequential([
    # Adds a densely-connected layer with 64 units to the model:
    layers.Dense(64, activation='relu'),
    # Add another:
    layers.Dense(64, activation='relu'),
    # Add a softmax layer with 10 output units:
    layers.Dense(10, activation='softmax')])

model.compile(optimizer=tf.train.AdamOptimizer(0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# tf.keras.Model.compile é‡‡ç”¨ä¸‰ä¸ªé‡è¦å‚æ•°ï¼š

# optimizerï¼šæ­¤å¯¹è±¡ä¼šæŒ‡å®šè®­ç»ƒè¿‡ç¨‹ã€‚ä» tf.train æ¨¡å—å‘å…¶ä¼ é€’ä¼˜åŒ–å™¨å®ä¾‹ï¼Œä¾‹å¦‚ tf.train.AdamOptimizerã€tf.train.RMSPropOptimizer æˆ– tf.train.GradientDescentOptimizerã€‚
# lossï¼šè¦åœ¨ä¼˜åŒ–æœŸé—´æœ€å°åŒ–çš„å‡½æ•°ã€‚å¸¸è§é€‰æ‹©åŒ…æ‹¬å‡æ–¹è¯¯å·®(mse)ã€categorical_crossentropy å’Œ binary_crossentropyã€‚æŸå¤±å‡½æ•°ç”±åç§°æˆ–é€šè¿‡ä» tf.keras.losses æ¨¡å—ä¼ é€’å¯è°ƒç”¨å¯¹è±¡æ¥æŒ‡å®šã€‚
# metricsï¼šç”¨äºç›‘æ§è®­ç»ƒã€‚å®ƒä»¬æ˜¯ tf.keras.metrics æ¨¡å—ä¸­çš„å­—ç¬¦ä¸²åç§°æˆ–å¯è°ƒç”¨å¯¹è±¡ã€‚
# Configure a model for mean-squared error regression.
model.compile(optimizer=tf.train.AdamOptimizer(0.01),
              loss='mse',       # mean squared error
              metrics=['mae'])  # mean absolute error

# Configure a model for categorical classification.
model.compile(optimizer=tf.train.RMSPropOptimizer(0.01),
              loss=tf.keras.losses.categorical_crossentropy,
              metrics=[tf.keras.metrics.categorical_accuracy])


data = np.random.random((1000, 32))
labels = np.random.random((1000, 10))

model.fit(data, labels, epochs=10, batch_size=32)
# tf.keras.Model.fit é‡‡ç”¨ä¸‰ä¸ªé‡è¦å‚æ•°ï¼š

# epochsï¼šä»¥å‘¨æœŸä¸ºå•ä½è¿›è¡Œè®­ç»ƒã€‚ä¸€ä¸ªå‘¨æœŸæ˜¯å¯¹æ•´ä¸ªè¾“å…¥æ•°æ®çš„ä¸€æ¬¡è¿­ä»£ï¼ˆä»¥è¾ƒå°çš„æ‰¹æ¬¡å®Œæˆè¿­ä»£ï¼‰ã€‚
# batch_sizeï¼šå½“ä¼ é€’ NumPy æ•°æ®æ—¶ï¼Œæ¨¡å‹å°†æ•°æ®åˆ†æˆè¾ƒå°çš„æ‰¹æ¬¡ï¼Œå¹¶åœ¨è®­ç»ƒæœŸé—´è¿­ä»£è¿™äº›æ‰¹æ¬¡ã€‚æ­¤æ•´æ•°æŒ‡å®šæ¯ä¸ªæ‰¹æ¬¡çš„å¤§å°ã€‚è¯·æ³¨æ„ï¼Œå¦‚æœæ ·æœ¬æ€»æ•°ä¸èƒ½è¢«æ‰¹æ¬¡å¤§å°æ•´é™¤ï¼Œåˆ™æœ€åä¸€ä¸ªæ‰¹æ¬¡å¯èƒ½æ›´å°ã€‚
# validation_dataï¼šåœ¨å¯¹æ¨¡å‹è¿›è¡ŒåŸå‹è®¾è®¡æ—¶ï¼Œæ‚¨éœ€è¦è½»æ¾ç›‘æ§è¯¥æ¨¡å‹åœ¨æŸäº›éªŒè¯æ•°æ®ä¸Šè¾¾åˆ°çš„æ•ˆæœã€‚ä¼ é€’æ­¤å‚æ•°ï¼ˆè¾“å…¥å’Œæ ‡ç­¾å…ƒç»„ï¼‰å¯ä»¥è®©è¯¥æ¨¡å‹åœ¨æ¯ä¸ªå‘¨æœŸç»“æŸæ—¶ä»¥æ¨ç†æ¨¡å¼æ˜¾ç¤ºæ‰€ä¼ é€’æ•°æ®çš„æŸå¤±å’ŒæŒ‡æ ‡ã€‚

data = np.random.random((1000, 32))
labels = np.random.random((1000, 10))

val_data = np.random.random((100, 32))
val_labels = np.random.random((100, 10))

model.fit(data, labels, epochs=10, batch_size=32,
          validation_data=(val_data, val_labels))

```